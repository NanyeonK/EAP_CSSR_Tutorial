{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YeonChan Kang\\anaconda3\\envs\\fdb\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import wrds\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import skew, kurtosis, pearsonr, spearmanr\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch7. The CRSP Sample and Market Factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cause the data is needed for portfolio analysis, we start from the Ch7.   \n",
    "CRSP data is neccesary for financial studieds 1925 through the present.   \n",
    "In the book, as in almost all studies of the U.S. stock market, we use the data from CRSP as our primary source for U.S. stock market data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WRDS recommends setting up a .pgpass file.\n",
      "Created .pgpass file successfully.\n",
      "You can create this file yourself at any time with the create_pgpass_file() function.\n",
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "db = wrds.Connection() # Connect to WRDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We splite the data sample along two dimension: Stock exchnge and Sector.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposing by Stock Exchange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXCHCD is the exchange code   \n",
    "1 = NYSE   \n",
    "2 = AMEX   \n",
    "3 = NASDAQ   \n",
    "4 = ARCA   \n",
    "   \n",
    "10 = Boston Stock Exchange   \n",
    "13 = Chicago stock Exxchange   \n",
    "16 = Pacific Stock Exchange   \n",
    "17 = Philadelphia Stock Exchange   \n",
    "19 = Toronto Stock Exchange   \n",
    "20 = OTC   \n",
    "   \n",
    "-2 = delisted   \n",
    "-1 = suspended   \n",
    "0 = not NYSE, AMEX, or NASDAQ   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately 5037353 rows in crsp.msf.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>nullable</th>\n",
       "      <th>type</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cusip</td>\n",
       "      <td>True</td>\n",
       "      <td>VARCHAR(8)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>permno</td>\n",
       "      <td>True</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>permco</td>\n",
       "      <td>True</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>issuno</td>\n",
       "      <td>True</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hexcd</td>\n",
       "      <td>True</td>\n",
       "      <td>SMALLINT</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hsiccd</td>\n",
       "      <td>True</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>date</td>\n",
       "      <td>True</td>\n",
       "      <td>DATE</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bidlo</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(11, 5)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>askhi</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(11, 5)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prc</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(11, 5)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>vol</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(10, 0)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ret</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(10, 6)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bid</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(11, 5)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ask</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(11, 5)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>shrout</td>\n",
       "      <td>True</td>\n",
       "      <td>DOUBLE PRECISION</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cfacpr</td>\n",
       "      <td>True</td>\n",
       "      <td>DOUBLE PRECISION</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cfacshr</td>\n",
       "      <td>True</td>\n",
       "      <td>DOUBLE PRECISION</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>altprc</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(11, 5)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>spread</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(10, 5)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>altprcdt</td>\n",
       "      <td>True</td>\n",
       "      <td>DATE</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>retx</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(10, 6)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name  nullable              type comment\n",
       "0      cusip      True        VARCHAR(8)    None\n",
       "1     permno      True           INTEGER    None\n",
       "2     permco      True           INTEGER    None\n",
       "3     issuno      True           INTEGER    None\n",
       "4      hexcd      True          SMALLINT    None\n",
       "5     hsiccd      True           INTEGER    None\n",
       "6       date      True              DATE    None\n",
       "7      bidlo      True    NUMERIC(11, 5)    None\n",
       "8      askhi      True    NUMERIC(11, 5)    None\n",
       "9        prc      True    NUMERIC(11, 5)    None\n",
       "10       vol      True    NUMERIC(10, 0)    None\n",
       "11       ret      True    NUMERIC(10, 6)    None\n",
       "12       bid      True    NUMERIC(11, 5)    None\n",
       "13       ask      True    NUMERIC(11, 5)    None\n",
       "14    shrout      True  DOUBLE PRECISION    None\n",
       "15    cfacpr      True  DOUBLE PRECISION    None\n",
       "16   cfacshr      True  DOUBLE PRECISION    None\n",
       "17    altprc      True    NUMERIC(11, 5)    None\n",
       "18    spread      True    NUMERIC(10, 5)    None\n",
       "19  altprcdt      True              DATE    None\n",
       "20      retx      True    NUMERIC(10, 6)    None"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.describe_table(library='crsp', table='msf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before 1963 July, CRSP doesn't contain AMEX.   \n",
    "1972 Nov, Nasdaq added at CRSP but NASDAQ stock appears in January 1969.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposting by industry sector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These securities can be identified using CRSP’s monthly stock names (msenames) file.   \n",
    "SHRCD = 10 or 11 is common stock   \n",
    "SICCD code standard industrial classification code   \n",
    "SIC CODE 0001-0999: Agriculture, Forestry and Fishing   \n",
    "SIC CODE 1000-1499: Mining   \n",
    "SIC CODE 1500-1799: Construction   \n",
    "SIC CODE 2000-3999: Manufacturing   \n",
    "SIC CODE 4000-4999: Transportation, Communications, Electric, Gas and Sanitary service   \n",
    "SIC CODE 5000-5199: Wholesale Trade   \n",
    "SIC CODE 5200-5999: Retail Trade   \n",
    "SIC CODE 6000-6799: Finance, Insurance and Real Estate   \n",
    "SIC CODE 7000-8999: Services   \n",
    "SIC CODE 9100-9729: Public Administration   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately 113856 rows in crsp.msenames.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>nullable</th>\n",
       "      <th>type</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>permno</td>\n",
       "      <td>True</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>namedt</td>\n",
       "      <td>True</td>\n",
       "      <td>DATE</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nameendt</td>\n",
       "      <td>True</td>\n",
       "      <td>DATE</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shrcd</td>\n",
       "      <td>True</td>\n",
       "      <td>SMALLINT</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>exchcd</td>\n",
       "      <td>True</td>\n",
       "      <td>SMALLINT</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>siccd</td>\n",
       "      <td>True</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ncusip</td>\n",
       "      <td>True</td>\n",
       "      <td>VARCHAR(8)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ticker</td>\n",
       "      <td>True</td>\n",
       "      <td>VARCHAR(8)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>comnam</td>\n",
       "      <td>True</td>\n",
       "      <td>VARCHAR(35)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>shrcls</td>\n",
       "      <td>True</td>\n",
       "      <td>VARCHAR(4)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tsymbol</td>\n",
       "      <td>True</td>\n",
       "      <td>VARCHAR(10)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>naics</td>\n",
       "      <td>True</td>\n",
       "      <td>VARCHAR(7)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>primexch</td>\n",
       "      <td>True</td>\n",
       "      <td>VARCHAR(1)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>trdstat</td>\n",
       "      <td>True</td>\n",
       "      <td>VARCHAR(1)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>secstat</td>\n",
       "      <td>True</td>\n",
       "      <td>VARCHAR(1)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>permco</td>\n",
       "      <td>True</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>compno</td>\n",
       "      <td>True</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>issuno</td>\n",
       "      <td>True</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hexcd</td>\n",
       "      <td>True</td>\n",
       "      <td>SMALLINT</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hsiccd</td>\n",
       "      <td>True</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cusip</td>\n",
       "      <td>True</td>\n",
       "      <td>VARCHAR(8)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name  nullable         type comment\n",
       "0     permno      True      INTEGER    None\n",
       "1     namedt      True         DATE    None\n",
       "2   nameendt      True         DATE    None\n",
       "3      shrcd      True     SMALLINT    None\n",
       "4     exchcd      True     SMALLINT    None\n",
       "5      siccd      True      INTEGER    None\n",
       "6     ncusip      True   VARCHAR(8)    None\n",
       "7     ticker      True   VARCHAR(8)    None\n",
       "8     comnam      True  VARCHAR(35)    None\n",
       "9     shrcls      True   VARCHAR(4)    None\n",
       "10   tsymbol      True  VARCHAR(10)    None\n",
       "11     naics      True   VARCHAR(7)    None\n",
       "12  primexch      True   VARCHAR(1)    None\n",
       "13   trdstat      True   VARCHAR(1)    None\n",
       "14   secstat      True   VARCHAR(1)    None\n",
       "15    permco      True      INTEGER    None\n",
       "16    compno      True      INTEGER    None\n",
       "17    issuno      True      INTEGER    None\n",
       "18     hexcd      True     SMALLINT    None\n",
       "19    hsiccd      True      INTEGER    None\n",
       "20     cusip      True   VARCHAR(8)    None"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.describe_table(library='crsp', table='msenames') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock returns and Excess return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, it is optimal to examine excess stock returns instead of simply the stock return.   \n",
    "Follow the book, we use risk-free rate from Kenneth French data   \n",
    "We express excess return as $r_{i,t}$   \n",
    "   \n",
    "For the most part, however, the cross-sectional distributions of unadjusted returns look very similar to the distributions of delisting-adjusted returns.   \n",
    "It is not surprising, therefore, that the choice of whether to use the unadjusted or adjusted returns has very little impact on the results of most empirical analyses.   \n",
    "That being said, to more accurately reflect the returns an investor would realize by making an investment at the end of month t and holding until the end of month t + 1, we use the delisting-adjusted future excess return (rt+1) as the focal return variable throughout this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately 1174 rows in ff.factors_monthly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>nullable</th>\n",
       "      <th>type</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>date</td>\n",
       "      <td>True</td>\n",
       "      <td>DATE</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mktrf</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(8, 6)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smb</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(8, 6)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hml</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(8, 6)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(7, 5)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>year</td>\n",
       "      <td>True</td>\n",
       "      <td>DOUBLE PRECISION</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>month</td>\n",
       "      <td>True</td>\n",
       "      <td>DOUBLE PRECISION</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>umd</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(8, 6)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dateff</td>\n",
       "      <td>True</td>\n",
       "      <td>DATE</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  nullable              type comment\n",
       "0    date      True              DATE    None\n",
       "1   mktrf      True     NUMERIC(8, 6)    None\n",
       "2     smb      True     NUMERIC(8, 6)    None\n",
       "3     hml      True     NUMERIC(8, 6)    None\n",
       "4      rf      True     NUMERIC(7, 5)    None\n",
       "5    year      True  DOUBLE PRECISION    None\n",
       "6   month      True  DOUBLE PRECISION    None\n",
       "7     umd      True     NUMERIC(8, 6)    None\n",
       "8  dateff      True              DATE    None"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kenneth French Data Library\n",
    "db.describe_table(library='ff', table='factors_monthly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately 25732 rows in ff.factors_daily.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>nullable</th>\n",
       "      <th>type</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>date</td>\n",
       "      <td>True</td>\n",
       "      <td>DATE</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mktrf</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(8, 6)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smb</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(8, 6)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hml</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(8, 6)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(7, 5)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>umd</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(8, 6)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  nullable           type comment\n",
       "0   date      True           DATE    None\n",
       "1  mktrf      True  NUMERIC(8, 6)    None\n",
       "2    smb      True  NUMERIC(8, 6)    None\n",
       "3    hml      True  NUMERIC(8, 6)    None\n",
       "4     rf      True  NUMERIC(7, 5)    None\n",
       "5    umd      True  NUMERIC(8, 6)    None"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kenneth French Data Library\n",
    "db.describe_table(library='ff', table='factors_daily')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only risk-free rate, market excess return\n",
    "rf_m = db.raw_sql(\"\"\"SELECT \n",
    "                        date, rf, mktrf\n",
    "                    FROM \n",
    "                        ff.factors_monthly\n",
    "                    WHERE date between '1988-01-01' and '2012-12-31' \n",
    "                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>rf</th>\n",
       "      <th>mktrf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1988-01-01</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1988-02-01</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1988-03-01</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>-0.0227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1988-04-01</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1988-05-01</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>-0.0029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>2012-08-01</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>2012-09-01</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>2012-10-01</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.0176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>2012-11-01</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>2012-12-01</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date      rf   mktrf\n",
       "0    1988-01-01  0.0029  0.0421\n",
       "1    1988-02-01  0.0046  0.0475\n",
       "2    1988-03-01  0.0044 -0.0227\n",
       "3    1988-04-01  0.0046  0.0056\n",
       "4    1988-05-01  0.0051 -0.0029\n",
       "..          ...     ...     ...\n",
       "295  2012-08-01  0.0001  0.0255\n",
       "296  2012-09-01  0.0001  0.0273\n",
       "297  2012-10-01  0.0001 -0.0176\n",
       "298  2012-11-01  0.0001  0.0078\n",
       "299  2012-12-01  0.0001  0.0118\n",
       "\n",
       "[300 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_d = db.raw_sql(\"\"\"SELECT date, rf, mktrf\n",
    "                    FROM ff.factors_daily\n",
    "                    WHERE date between '1988-01-01' and '2012-12-31' \n",
    "                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>rf</th>\n",
       "      <th>mktrf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1988-01-04</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>0.0334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1988-01-05</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>0.0122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1988-01-06</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>0.0027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1988-01-07</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>0.0074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1988-01-08</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>-0.0566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6297</th>\n",
       "      <td>2012-12-24</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>-0.0024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6298</th>\n",
       "      <td>2012-12-26</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>-0.0054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6299</th>\n",
       "      <td>2012-12-27</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>-0.0011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6300</th>\n",
       "      <td>2012-12-28</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>-0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6301</th>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.0171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6302 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date       rf   mktrf\n",
       "0     1988-01-04  0.00015  0.0334\n",
       "1     1988-01-05  0.00015  0.0122\n",
       "2     1988-01-06  0.00015  0.0027\n",
       "3     1988-01-07  0.00015  0.0074\n",
       "4     1988-01-08  0.00015 -0.0566\n",
       "...          ...      ...     ...\n",
       "6297  2012-12-24  0.00001 -0.0024\n",
       "6298  2012-12-26  0.00001 -0.0054\n",
       "6299  2012-12-27  0.00001 -0.0011\n",
       "6300  2012-12-28  0.00001 -0.0100\n",
       "6301  2012-12-31  0.00001  0.0171\n",
       "\n",
       "[6302 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delisted Stock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, it is optimal to examine excess stock returns instead of simply the stock return.\n",
    "We can use risk-free rate from Kenneth French data\n",
    "We express excess return as $r_{i,t}$\n",
    "\n",
    "For the most part, however, the cross-sectional distributions of unadjusted returns look very similar to the distributions of delisting-adjusted returns. It is not surprising, therefore, that the choice of whether to use the unadjusted or adjusted returns has very little impact on the results of most empirical analyses. That being said, to more accurately reflect the returns an investor would realize by making an investment at the end of month t and holding until the end of month t + 1, we use the delisting-adjusted future excess return (rt+1) as the focal return variable throughout this text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifier and characteristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the description of the table crsp.msf   \n",
    "This table contains monthly stock data for all CRSP-listed securities.   \n",
    "   \n",
    "The most used columns are:   \n",
    "- SHROUT is the number of shares outstanding in thousands\n",
    "- ALTPRC is the adjusted price. Take absolute value of ALTPRC to get the price \n",
    "- MarketCap = SHROUT * ALTPRC / 1000\n",
    "- Ret is the return of the stock\n",
    "The exceptions to this are the monthly return when a stock delists from an exchange.   \n",
    "   \n",
    "I usally we only consider stocks with EXCHCD = 1, 2, 3, and the others are considered as 0\n",
    "db.describe_table(library='crsp', table='msf')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately 37747 rows in crsp.msedelist.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>nullable</th>\n",
       "      <th>type</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>permno</td>\n",
       "      <td>True</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dlstdt</td>\n",
       "      <td>True</td>\n",
       "      <td>DATE</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dlstcd</td>\n",
       "      <td>True</td>\n",
       "      <td>SMALLINT</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nwperm</td>\n",
       "      <td>True</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nwcomp</td>\n",
       "      <td>True</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nextdt</td>\n",
       "      <td>True</td>\n",
       "      <td>DATE</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dlamt</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(11, 5)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dlretx</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(10, 6)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dlprc</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(11, 5)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dlpdt</td>\n",
       "      <td>True</td>\n",
       "      <td>DATE</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dlret</td>\n",
       "      <td>True</td>\n",
       "      <td>NUMERIC(10, 6)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>permco</td>\n",
       "      <td>True</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>compno</td>\n",
       "      <td>True</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>issuno</td>\n",
       "      <td>True</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hexcd</td>\n",
       "      <td>True</td>\n",
       "      <td>SMALLINT</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hsiccd</td>\n",
       "      <td>True</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cusip</td>\n",
       "      <td>True</td>\n",
       "      <td>VARCHAR(8)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>acperm</td>\n",
       "      <td>True</td>\n",
       "      <td>DOUBLE PRECISION</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>accomp</td>\n",
       "      <td>True</td>\n",
       "      <td>DOUBLE PRECISION</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name  nullable              type comment\n",
       "0   permno      True           INTEGER    None\n",
       "1   dlstdt      True              DATE    None\n",
       "2   dlstcd      True          SMALLINT    None\n",
       "3   nwperm      True           INTEGER    None\n",
       "4   nwcomp      True           INTEGER    None\n",
       "5   nextdt      True              DATE    None\n",
       "6    dlamt      True    NUMERIC(11, 5)    None\n",
       "7   dlretx      True    NUMERIC(10, 6)    None\n",
       "8    dlprc      True    NUMERIC(11, 5)    None\n",
       "9    dlpdt      True              DATE    None\n",
       "10   dlret      True    NUMERIC(10, 6)    None\n",
       "11  permco      True           INTEGER    None\n",
       "12  compno      True           INTEGER    None\n",
       "13  issuno      True           INTEGER    None\n",
       "14   hexcd      True          SMALLINT    None\n",
       "15  hsiccd      True           INTEGER    None\n",
       "16   cusip      True        VARCHAR(8)    None\n",
       "17  acperm      True  DOUBLE PRECISION    None\n",
       "18  accomp      True  DOUBLE PRECISION    None"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.describe_table(library='crsp', table='msedelist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifier\n",
    "- Cusip  (VARCHAR(8))       : 8-character identifier for securities, with issuer (6 chars) and issue (2 chars), including dummy codes assigned by CRSP.   \n",
    "- PERMCO (INTEGER)          : Unique, permanent identifier for companies on CRSP files, with values under 20,000 indicating Nasdaq-assigned numbers.   \n",
    "- ISSUNO (INTEGER)          : Unique identifier for each Nasdaq-listed security, set to zero if unknown, may change if Nasdaq reassigns the number.   \n",
    "- HEXCD  (INTEGER)          : Latest exchange code for a security, with 1 for NYSE, 2 for AMEX, and 3 for Nasdaq.   \n",
    "- HSICCD (INTEGER)          : Latest non-zero Standard Industrial Classification (SIC) Code for a security; zero if not provided by CRSP's data source.   \n",
    "\n",
    "##### Date\n",
    "- DATE (DATE)               : Date of the observation, in the form YYYY-MM-DD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characteristics\n",
    "- PRC   (NUMERIC(11,5))     : Closing price or negative bid/ask average for a trading day; set to zero if unavailable; monthly data shows last trading date price.   \n",
    "- VOL   (NUMERIC(10,0))     : Trading volume, expressed in units of one share for daily data and hundreds for monthly data, set to -99 if missing, with zero indicating no trades.   \n",
    "- RET   (NUMERIC(10,6))     : Holding period return, calculated as (p(t)f(t)+d(t))/p(t')-1, with special codes indicating reasons for missing returns.   \n",
    "- BID   (NUMERIC(11,5))     : Closing bid price for NYSE, AMEX, and NASDAQ securities; NASDAQ uses inside quotations since July 1980, while NYSE/AMEX uses the last representative quote.   \n",
    "- ASK   (NUMERIC(11,5))     : Closing ask price for NYSE, AMEX, and NASDAQ securities; NASDAQ uses inside quotations since July 1980, while NYSE/AMEX uses the last representative quote.   \n",
    "- SHROUT (DOUBLE PRECISION) : Number of publicly held shares, recorded in thousands.   \n",
    "- BIDLO (NUMERIC(11,5))     : Daily lowest trading or closing bid price, set to zero if unavailable; monthly files show lowest daily price or bid/ask average.   \n",
    "- ASKHI (NUMERIC(11,5))     : Daily highest trading or closing ask price, set to zero if unavailable; monthly files show highest daily price or bid/ask average.   \n",
    "- CFACPR(DOUBLE PRECISION)  : Cumulative factor to adjust price.   \n",
    "- CFACSHR(DOUBLE PRECISION) : Cumulative factor to adjust shares outstanding.   \n",
    "- ALTPRC(NUMERIC(11,5))     : Alternate monthly price from daily prices, showing the last non-missing price in the month; available only on monthly databases.   \n",
    "- SPREAD(NUMERIC(10,5))     : Monthly difference between closing bid and ask quotes, set to zero if unavailable, with sign indicating bid/ask status when Closing Price or Bid/Ask Average is zero.   \n",
    "- ALTPRCDT(DATE)            : Date of the monthly Price Alternate; set to zero if no non-missing prices in the month.   \n",
    "- RETX(NUMERIC(10,6))       : Holding period return excluding dividends, calculated similarly to RET but typically with dividends (d(t)) set to zero.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use altprc instead of prc to get the adjusted price\n",
    "sql = \"\"\"SELECT date, permno, altprc, ret, shrout, vol\n",
    "        FROM crsp.msf \n",
    "        WHERE date between '1988-01-01' and '2012-12-31'\n",
    "        \"\"\"\n",
    "crsp_m = db.raw_sql(sql, date_cols=['date'], chunksize=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>permno</th>\n",
       "      <th>altprc</th>\n",
       "      <th>ret</th>\n",
       "      <th>shrout</th>\n",
       "      <th>vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1988-01-29</td>\n",
       "      <td>10001</td>\n",
       "      <td>6.2500</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>992.0</td>\n",
       "      <td>490.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1988-02-29</td>\n",
       "      <td>10001</td>\n",
       "      <td>6.7500</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>992.0</td>\n",
       "      <td>382.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1988-03-31</td>\n",
       "      <td>10001</td>\n",
       "      <td>6.1250</td>\n",
       "      <td>-0.076296</td>\n",
       "      <td>992.0</td>\n",
       "      <td>369.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1988-04-29</td>\n",
       "      <td>10001</td>\n",
       "      <td>-6.3125</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>992.0</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1988-05-31</td>\n",
       "      <td>10001</td>\n",
       "      <td>-6.4375</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>992.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74724</th>\n",
       "      <td>2012-08-31</td>\n",
       "      <td>93436</td>\n",
       "      <td>28.5200</td>\n",
       "      <td>0.040117</td>\n",
       "      <td>105432.0</td>\n",
       "      <td>247781.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74725</th>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>93436</td>\n",
       "      <td>29.2800</td>\n",
       "      <td>0.026648</td>\n",
       "      <td>105772.0</td>\n",
       "      <td>335868.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74726</th>\n",
       "      <td>2012-10-31</td>\n",
       "      <td>93436</td>\n",
       "      <td>28.1314</td>\n",
       "      <td>-0.039228</td>\n",
       "      <td>113779.0</td>\n",
       "      <td>178360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74727</th>\n",
       "      <td>2012-11-30</td>\n",
       "      <td>93436</td>\n",
       "      <td>33.8200</td>\n",
       "      <td>0.202215</td>\n",
       "      <td>113779.0</td>\n",
       "      <td>224326.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74728</th>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>93436</td>\n",
       "      <td>33.8700</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>114214.0</td>\n",
       "      <td>217520.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2274729 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  permno   altprc       ret    shrout       vol\n",
       "0     1988-01-29   10001   6.2500  0.063830     992.0     490.0\n",
       "1     1988-02-29   10001   6.7500  0.080000     992.0     382.0\n",
       "2     1988-03-31   10001   6.1250 -0.076296     992.0     369.0\n",
       "3     1988-04-29   10001  -6.3125  0.030612     992.0     121.0\n",
       "4     1988-05-31   10001  -6.4375  0.019802     992.0     102.0\n",
       "...          ...     ...      ...       ...       ...       ...\n",
       "74724 2012-08-31   93436  28.5200  0.040117  105432.0  247781.0\n",
       "74725 2012-09-28   93436  29.2800  0.026648  105772.0  335868.0\n",
       "74726 2012-10-31   93436  28.1314 -0.039228  113779.0  178360.0\n",
       "74727 2012-11-30   93436  33.8200  0.202215  113779.0  224326.0\n",
       "74728 2012-12-31   93436  33.8700  0.001478  114214.0  217520.0\n",
       "\n",
       "[2274729 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crsp_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no altprc in crsp.dsf\n",
    "crsp_d = db.raw_sql(\"\"\"SELECT date, permno, prc, ret, shrout, vol\n",
    "                      FROM crsp.dsf \n",
    "                      WHERE date between '1988-01-01' and '2012-12-31'\n",
    "                  \"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crsp_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_types(df):\n",
    "    \"\"\"\n",
    "    Optimizes the data types of a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to be optimized.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with optimized data types.\n",
    "    \"\"\"\n",
    "    changes = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        original_dtype = df[col].dtype\n",
    "        \n",
    "        # Convert date columns to datetime type\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[col]) or 'date' in col.lower():\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        \n",
    "        # Convert integer-like columns to appropriate integer type\n",
    "        elif pd.api.types.is_integer_dtype(df[col]):\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer', errors='coerce')\n",
    "            # Downcast to smallest possible integer type\n",
    "            if df[col].max(skipna=True) <= np.iinfo(np.int16).max and df[col].min(skipna=True) >= np.iinfo(np.int16).min:\n",
    "                df[col] = df[col].astype(pd.Int16Dtype())\n",
    "            elif df[col].max(skipna=True) <= np.iinfo(np.int32).max and df[col].min(skipna=True) >= np.iinfo(np.int32).min:\n",
    "                df[col] = df[col].astype(pd.Int32Dtype())\n",
    "        \n",
    "        # Convert float-like columns to appropriate float type\n",
    "        elif pd.api.types.is_float_dtype(df[col]):\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float', errors='coerce')\n",
    "            \n",
    "            # Check if all values are integers (float ending with .0)\n",
    "            finite_values = df[col][np.isfinite(df[col])]\n",
    "            if all(finite_values == finite_values.astype(np.int64)):\n",
    "                df[col] = df[col].astype(pd.Int64Dtype())\n",
    "                # Downcast to smallest possible integer type\n",
    "                if df[col].max(skipna=True) <= np.iinfo(np.int16).max and df[col].min(skipna=True) >= np.iinfo(np.int16).min:\n",
    "                    df[col] = df[col].astype(pd.Int16Dtype())\n",
    "                elif df[col].max(skipna=True) <= np.iinfo(np.int32).max and df[col].min(skipna=True) >= np.iinfo(np.int32).min:\n",
    "                    df[col] = df[col].astype(pd.Int32Dtype())\n",
    "            else:\n",
    "                # Downcast to smallest possible float type\n",
    "                if df[col].max(skipna=True) <= np.finfo(np.float16).max and df[col].min(skipna=True) >= np.finfo(np.float16).min:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif df[col].max(skipna=True) <= np.finfo(np.float32).max and df[col].min(skipna=True) >= np.finfo(np.float32).min:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "        \n",
    "        # Convert object columns to string type\n",
    "        elif pd.api.types.is_object_dtype(df[col]):\n",
    "            df[col] = df[col].astype(str)\n",
    "        \n",
    "        # Record changes in column data types\n",
    "        if df[col].dtype != original_dtype:\n",
    "            changes.append(f\"'{col}' changed from {original_dtype} to {df[col].dtype}\")\n",
    "    \n",
    "    # Print changes in column data types\n",
    "    if changes:\n",
    "        for change in changes:\n",
    "            print(change)\n",
    "    else:\n",
    "        print(\"No changes were made.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YeonChan Kang\\anaconda3\\envs\\fdb\\lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "crsp_m['altprc'] = crsp_m['altprc'].abs() # abs(prc) If there is not a close price, the mid price of bid and ask is used with a negative sign. Change them to a positive value.\n",
    "crsp_m['mcap'] = crsp_m.altprc * crsp_m.shrout # Market capitalization\n",
    "crsp_m['size'] = np.log(crsp_m['mcap']) # Size is the log of market capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>permno</th>\n",
       "      <th>altprc</th>\n",
       "      <th>ret</th>\n",
       "      <th>shrout</th>\n",
       "      <th>vol</th>\n",
       "      <th>mcap</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1988-01-29</td>\n",
       "      <td>10001</td>\n",
       "      <td>6.2500</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>992.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>6.200000e+03</td>\n",
       "      <td>8.732305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1988-02-29</td>\n",
       "      <td>10001</td>\n",
       "      <td>6.7500</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>992.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>6.696000e+03</td>\n",
       "      <td>8.809266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1988-03-31</td>\n",
       "      <td>10001</td>\n",
       "      <td>6.1250</td>\n",
       "      <td>-0.076296</td>\n",
       "      <td>992.0</td>\n",
       "      <td>369.0</td>\n",
       "      <td>6.076000e+03</td>\n",
       "      <td>8.712102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1988-04-29</td>\n",
       "      <td>10001</td>\n",
       "      <td>6.3125</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>992.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>6.262000e+03</td>\n",
       "      <td>8.742255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1988-05-31</td>\n",
       "      <td>10001</td>\n",
       "      <td>6.4375</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>992.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>6.386000e+03</td>\n",
       "      <td>8.761863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74724</th>\n",
       "      <td>2012-08-31</td>\n",
       "      <td>93436</td>\n",
       "      <td>28.5200</td>\n",
       "      <td>0.040117</td>\n",
       "      <td>105432.0</td>\n",
       "      <td>247781.0</td>\n",
       "      <td>3.006921e+06</td>\n",
       "      <td>14.916427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74725</th>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>93436</td>\n",
       "      <td>29.2800</td>\n",
       "      <td>0.026648</td>\n",
       "      <td>105772.0</td>\n",
       "      <td>335868.0</td>\n",
       "      <td>3.097004e+06</td>\n",
       "      <td>14.945946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74726</th>\n",
       "      <td>2012-10-31</td>\n",
       "      <td>93436</td>\n",
       "      <td>28.1314</td>\n",
       "      <td>-0.039228</td>\n",
       "      <td>113779.0</td>\n",
       "      <td>178360.0</td>\n",
       "      <td>3.200763e+06</td>\n",
       "      <td>14.978900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74727</th>\n",
       "      <td>2012-11-30</td>\n",
       "      <td>93436</td>\n",
       "      <td>33.8200</td>\n",
       "      <td>0.202215</td>\n",
       "      <td>113779.0</td>\n",
       "      <td>224326.0</td>\n",
       "      <td>3.848006e+06</td>\n",
       "      <td>15.163066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74728</th>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>93436</td>\n",
       "      <td>33.8700</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>114214.0</td>\n",
       "      <td>217520.0</td>\n",
       "      <td>3.868428e+06</td>\n",
       "      <td>15.168359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2274729 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  permno   altprc       ret    shrout       vol          mcap  \\\n",
       "0     1988-01-29   10001   6.2500  0.063830     992.0     490.0  6.200000e+03   \n",
       "1     1988-02-29   10001   6.7500  0.080000     992.0     382.0  6.696000e+03   \n",
       "2     1988-03-31   10001   6.1250 -0.076296     992.0     369.0  6.076000e+03   \n",
       "3     1988-04-29   10001   6.3125  0.030612     992.0     121.0  6.262000e+03   \n",
       "4     1988-05-31   10001   6.4375  0.019802     992.0     102.0  6.386000e+03   \n",
       "...          ...     ...      ...       ...       ...       ...           ...   \n",
       "74724 2012-08-31   93436  28.5200  0.040117  105432.0  247781.0  3.006921e+06   \n",
       "74725 2012-09-28   93436  29.2800  0.026648  105772.0  335868.0  3.097004e+06   \n",
       "74726 2012-10-31   93436  28.1314 -0.039228  113779.0  178360.0  3.200763e+06   \n",
       "74727 2012-11-30   93436  33.8200  0.202215  113779.0  224326.0  3.848006e+06   \n",
       "74728 2012-12-31   93436  33.8700  0.001478  114214.0  217520.0  3.868428e+06   \n",
       "\n",
       "            size  \n",
       "0       8.732305  \n",
       "1       8.809266  \n",
       "2       8.712102  \n",
       "3       8.742255  \n",
       "4       8.761863  \n",
       "...          ...  \n",
       "74724  14.916427  \n",
       "74725  14.945946  \n",
       "74726  14.978900  \n",
       "74727  15.163066  \n",
       "74728  15.168359  \n",
       "\n",
       "[2274729 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crsp_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes were made.\n"
     ]
    }
   ],
   "source": [
    "crsp_m = convert_types(crsp_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are so many missing values in the dataset.\n",
    "# We use the dropna method to remove the missing values.\n",
    "crsp_m.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>permno</th>\n",
       "      <th>altprc</th>\n",
       "      <th>ret</th>\n",
       "      <th>shrout</th>\n",
       "      <th>vol</th>\n",
       "      <th>mcap</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1988-01-29</td>\n",
       "      <td>10001</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>0.063843</td>\n",
       "      <td>992</td>\n",
       "      <td>490</td>\n",
       "      <td>6200.00</td>\n",
       "      <td>8.732305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1988-02-29</td>\n",
       "      <td>10001</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>0.080017</td>\n",
       "      <td>992</td>\n",
       "      <td>382</td>\n",
       "      <td>6696.00</td>\n",
       "      <td>8.809265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1988-03-31</td>\n",
       "      <td>10001</td>\n",
       "      <td>6.125000</td>\n",
       "      <td>-0.076294</td>\n",
       "      <td>992</td>\n",
       "      <td>369</td>\n",
       "      <td>6076.00</td>\n",
       "      <td>8.712102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1988-04-29</td>\n",
       "      <td>10001</td>\n",
       "      <td>6.312500</td>\n",
       "      <td>0.030609</td>\n",
       "      <td>992</td>\n",
       "      <td>121</td>\n",
       "      <td>6262.00</td>\n",
       "      <td>8.742255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1988-05-31</td>\n",
       "      <td>10001</td>\n",
       "      <td>6.437500</td>\n",
       "      <td>0.019806</td>\n",
       "      <td>992</td>\n",
       "      <td>102</td>\n",
       "      <td>6386.00</td>\n",
       "      <td>8.761864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74724</th>\n",
       "      <td>2012-08-31</td>\n",
       "      <td>93436</td>\n",
       "      <td>28.520000</td>\n",
       "      <td>0.040131</td>\n",
       "      <td>105432</td>\n",
       "      <td>247781</td>\n",
       "      <td>3006920.75</td>\n",
       "      <td>14.916427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74725</th>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>93436</td>\n",
       "      <td>29.280001</td>\n",
       "      <td>0.026642</td>\n",
       "      <td>105772</td>\n",
       "      <td>335868</td>\n",
       "      <td>3097004.25</td>\n",
       "      <td>14.945946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74726</th>\n",
       "      <td>2012-10-31</td>\n",
       "      <td>93436</td>\n",
       "      <td>28.131399</td>\n",
       "      <td>-0.039215</td>\n",
       "      <td>113779</td>\n",
       "      <td>178360</td>\n",
       "      <td>3200762.50</td>\n",
       "      <td>14.978900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74727</th>\n",
       "      <td>2012-11-30</td>\n",
       "      <td>93436</td>\n",
       "      <td>33.820000</td>\n",
       "      <td>0.202271</td>\n",
       "      <td>113779</td>\n",
       "      <td>224326</td>\n",
       "      <td>3848005.75</td>\n",
       "      <td>15.163066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74728</th>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>93436</td>\n",
       "      <td>33.869999</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>114214</td>\n",
       "      <td>217520</td>\n",
       "      <td>3868428.25</td>\n",
       "      <td>15.168359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2190574 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  permno     altprc       ret  shrout     vol        mcap  \\\n",
       "0     1988-01-29   10001   6.250000  0.063843     992     490     6200.00   \n",
       "1     1988-02-29   10001   6.750000  0.080017     992     382     6696.00   \n",
       "2     1988-03-31   10001   6.125000 -0.076294     992     369     6076.00   \n",
       "3     1988-04-29   10001   6.312500  0.030609     992     121     6262.00   \n",
       "4     1988-05-31   10001   6.437500  0.019806     992     102     6386.00   \n",
       "...          ...     ...        ...       ...     ...     ...         ...   \n",
       "74724 2012-08-31   93436  28.520000  0.040131  105432  247781  3006920.75   \n",
       "74725 2012-09-28   93436  29.280001  0.026642  105772  335868  3097004.25   \n",
       "74726 2012-10-31   93436  28.131399 -0.039215  113779  178360  3200762.50   \n",
       "74727 2012-11-30   93436  33.820000  0.202271  113779  224326  3848005.75   \n",
       "74728 2012-12-31   93436  33.869999  0.001478  114214  217520  3868428.25   \n",
       "\n",
       "            size  \n",
       "0       8.732305  \n",
       "1       8.809265  \n",
       "2       8.712102  \n",
       "3       8.742255  \n",
       "4       8.761864  \n",
       "...          ...  \n",
       "74724  14.916427  \n",
       "74725  14.945946  \n",
       "74726  14.978900  \n",
       "74727  15.163066  \n",
       "74728  15.168359  \n",
       "\n",
       "[2190574 rows x 8 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crsp_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch1. Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Winsorization and Truncation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical asset pricing researchers usually take a more ad hoc approach to dealing with the effect of outliers   \n",
    "\n",
    "Two techniques are commonly used in empirical asset pricing research to deal with the effect of outliers: winsorizing and truncating.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Winsorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winsorizing is a technique that replaces extreme values with the nearest non-extreme value.   \n",
    "The idea is to replace the extreme values with the nearest non-extreme value.   \n",
    "For example, if the 1% quantile is 0.5 and the 99% quantile is 100, then all values less than 0.5 are replaced with 0.5 and all values greater than 100 are replaced with 100.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorize(data, column=False, lower=0.01, upper=0.99, copy=True):\n",
    "    \"\"\"\n",
    "    Winsorizes the input data by replacing extreme values with the nearest values within the specified quantiles.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.Series or pd.DataFrame): The data to be winsorized.\n",
    "    lower (float): The lower quantile threshold. Defaults to 0.01.\n",
    "    upper (float): The upper quantile threshold. Defaults to 0.99.\n",
    "    copy (bool): Whether to return a copy of the data or to modify it in place. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series or pd.DataFrame: The winsorized data.\n",
    "    \"\"\"\n",
    "    print(f\"Data winsorized between {lower} and {upper}\\n\")\n",
    "    \n",
    "    if copy:\n",
    "        data = data.copy()\n",
    "    \n",
    "    if column:\n",
    "        data = data[column]\n",
    "    \n",
    "    print(f\"Original maximum: {data.max()} and Original minimum: {data.min()}\\n\")\n",
    "    \n",
    "    qtl = data.quantile([lower, upper])\n",
    "    \n",
    "    # Replace values below3 the lower quantile\n",
    "    data[data < qtl.loc[lower]] = qtl.loc[lower]\n",
    "    \n",
    "    # Replace values above the upper quantile\n",
    "    data[data > qtl.loc[upper]] = qtl.loc[upper]\n",
    "    \n",
    "    \n",
    "    print(f\"New maximum: {data.max()} and New minimum: {data.min()}\\n\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data winsorized between 0.01 and 0.99\n",
      "\n",
      "Original maximum: 24.0 and Original minimum: -0.98828125\n",
      "\n",
      "New maximum: 0.5966796875 and New minimum: -0.41015625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "winsor_data = crsp_m.copy()\n",
    "winsor_data['ret'] = winsorize(crsp_m['ret']) # Winsorize returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncating is a technique that removes extreme values.   \n",
    "The idea is to remove the extreme values from the data.   \n",
    "For example, if the 1% quantile is 0.5 and the 99% quantile is 100, then all values less than 0.5 and all values greater than 100 are removed.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(data, column=False, lower=0.01, upper=0.99, copy=True):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        data (_type_): The data to be truncated.\n",
    "        lower (float): The lower quantile threshold. Defaults to 0.01.\n",
    "        upper (float): The upper quantile threshold. Defaults to 0.99.\n",
    "        copy (bool): Whether to return a copy of the data or to modify it in place. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series or pd.DataFrame: The truncated data.\n",
    "    \"\"\"\n",
    "    print(f\"Data winsorized between {lower} and {upper}\\n\")\n",
    "    \n",
    "    if copy:\n",
    "        data = data.copy()\n",
    "        \n",
    "    if column:\n",
    "        data = data[column]\n",
    "    \n",
    "    print(f\"Original maximum: {data.max()} and Original minimum: {data.min()}\\n\")\n",
    "    \n",
    "    qtl = data.quantile([lower, upper])\n",
    "    \n",
    "    # Remove values below the lower quantile\n",
    "    data = data[data >= qtl.loc[lower]]\n",
    "    \n",
    "    # Remove values above the upper quantile\n",
    "    data = data[data <= qtl.loc[upper]]\n",
    "    \n",
    "    print(f\"New maximum: {data.max()} and New minimum: {data.min()}\\n\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data winsorized between 0.01 and 0.99\n",
      "\n",
      "Original maximum: ret    24.0\n",
      "dtype: float16 and Original minimum: ret   -0.988281\n",
      "dtype: float16\n",
      "\n",
      "New maximum: ret    0.59668\n",
      "dtype: float16 and New minimum: ret   -0.410156\n",
      "dtype: float16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "truncate_data = crsp_m.copy()\n",
    "truncate_data['ret'] = truncate(crsp_m, ['ret']) # Winsorize returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEWEY AND WEST (1987) ADJUSTMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newey-West standard errors are a robust method for estimating the standard errors of the coefficients in a regression model.   \n",
    "The selection of the lag length is important because it determines the number of periods over which the autocorrelation of the residuals is calculated.   \n",
    "Usally, the lag length is set to the integer part of the cube root of the number of observations or 6 or 12 in empirical asset pricing research.   \n",
    "The Newey-West standard errors are calculated as follows:\n",
    "- 1. Estimate the regression model.\n",
    "- 2. Calculate the residuals.\n",
    "- 3. Calculate the autocovariance of the residuals.\n",
    "- 4. Calculate the Newey-West standard errors.\n",
    "   \n",
    "The Newey-West standard errors are robust to autocorrelation and heteroskedasticity in the residuals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newey_west_regression(y, X, lags=None):\n",
    "    \"\"\"\n",
    "    Performs a regression and calculates Newey-West standard errors, t-statistics, and p-values.\n",
    "    \n",
    "    Args:\n",
    "        y (pd.Series or np.array): Dependent variable.\n",
    "        X (pd.DataFrame or np.array): Independent variables.\n",
    "        lags (int): The lag length for Newey-West standard errors. If None, it will be set to the integer part of the cube root of the number of observations.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing coefficients, Newey-West standard errors, t-statistics, and p-values.\n",
    "    \"\"\"\n",
    "    if lags is None:\n",
    "        lags = int(np.floor(len(y)**(1/3)))  # Set lag length to the integer part of the cube root of the number of observations\n",
    "\n",
    "    # Ensure the dependent variable is numeric\n",
    "    y = pd.to_numeric(y, errors='coerce')\n",
    "    \n",
    "    # Ensure all independent variables are numeric\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Drop rows with any NaN values in y or X\n",
    "    valid_idx = ~y.isna() & X.notna().all(axis=1)\n",
    "    y = y[valid_idx]\n",
    "    X = X[valid_idx]\n",
    "    \n",
    "    # Add a constant term to the independent variables matrix\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Estimate the regression model\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    # Calculate Newey-West standard errors\n",
    "    robust_cov = model.get_robustcov_results(cov_type='HAC', maxlags=lags)\n",
    "    \n",
    "    # Extract coefficients, standard errors, t-statistics, and p-values\n",
    "    results = pd.DataFrame({\n",
    "        'Coefficient': model.params,\n",
    "        'Newey-West SE': robust_cov.bse,\n",
    "        't-Statistic': robust_cov.tvalues,\n",
    "        'p-Value': robust_cov.pvalues\n",
    "    })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = crsp_m.drop(['ret', 'date', 'permno'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnewey_west_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrsp_m\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mret\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[61], line 31\u001b[0m, in \u001b[0;36mnewey_west_regression\u001b[1;34m(y, X, lags)\u001b[0m\n\u001b[0;32m     28\u001b[0m X \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39madd_constant(X)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Estimate the regression model\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOLS\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Calculate Newey-West standard errors\u001b[39;00m\n\u001b[0;32m     34\u001b[0m robust_cov \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_robustcov_results(cov_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHAC\u001b[39m\u001b[38;5;124m'\u001b[39m, maxlags\u001b[38;5;241m=\u001b[39mlags)\n",
      "File \u001b[1;32mc:\\Users\\YeonChan Kang\\anaconda3\\envs\\fdb\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:922\u001b[0m, in \u001b[0;36mOLS.__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    919\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights are not supported in OLS and will be ignored\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    920\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn exception will be raised in the next version.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    921\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, ValueWarning)\n\u001b[1;32m--> 922\u001b[0m \u001b[38;5;28msuper\u001b[39m(OLS, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, missing\u001b[38;5;241m=\u001b[39mmissing,\n\u001b[0;32m    923\u001b[0m                           hasconst\u001b[38;5;241m=\u001b[39mhasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys:\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\YeonChan Kang\\anaconda3\\envs\\fdb\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:748\u001b[0m, in \u001b[0;36mWLS.__init__\u001b[1;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    747\u001b[0m     weights \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m--> 748\u001b[0m \u001b[38;5;28msuper\u001b[39m(WLS, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, missing\u001b[38;5;241m=\u001b[39mmissing,\n\u001b[0;32m    749\u001b[0m                           weights\u001b[38;5;241m=\u001b[39mweights, hasconst\u001b[38;5;241m=\u001b[39mhasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    750\u001b[0m nobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    751\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\n",
      "File \u001b[1;32mc:\\Users\\YeonChan Kang\\anaconda3\\envs\\fdb\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:202\u001b[0m, in \u001b[0;36mRegressionModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28msuper\u001b[39m(RegressionModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpinv_wexog: Float64Array \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_attr\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpinv_wexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwendog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\YeonChan Kang\\anaconda3\\envs\\fdb\\lib\\site-packages\\statsmodels\\base\\model.py:270\u001b[0m, in \u001b[0;36mLikelihoodModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n",
      "File \u001b[1;32mc:\\Users\\YeonChan Kang\\anaconda3\\envs\\fdb\\lib\\site-packages\\statsmodels\\base\\model.py:95\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m missing \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     94\u001b[0m hasconst \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhasconst\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_data(endog, exog, missing, hasconst,\n\u001b[0;32m     96\u001b[0m                               \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mk_constant\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexog\n",
      "File \u001b[1;32mc:\\Users\\YeonChan Kang\\anaconda3\\envs\\fdb\\lib\\site-packages\\statsmodels\\base\\model.py:135\u001b[0m, in \u001b[0;36mModel._handle_data\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 135\u001b[0m     data \u001b[38;5;241m=\u001b[39m handle_data(endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32mc:\\Users\\YeonChan Kang\\anaconda3\\envs\\fdb\\lib\\site-packages\\statsmodels\\base\\data.py:675\u001b[0m, in \u001b[0;36mhandle_data\u001b[1;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    672\u001b[0m     exog \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[0;32m    674\u001b[0m klass \u001b[38;5;241m=\u001b[39m handle_data_class_factory(endog, exog)\n\u001b[1;32m--> 675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m klass(endog, exog\u001b[38;5;241m=\u001b[39mexog, missing\u001b[38;5;241m=\u001b[39mmissing, hasconst\u001b[38;5;241m=\u001b[39mhasconst,\n\u001b[0;32m    676\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\YeonChan Kang\\anaconda3\\envs\\fdb\\lib\\site-packages\\statsmodels\\base\\data.py:84\u001b[0m, in \u001b[0;36mModelData.__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_endog \u001b[38;5;241m=\u001b[39m endog\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_exog \u001b[38;5;241m=\u001b[39m exog\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendog, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_endog_exog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconst_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\YeonChan Kang\\anaconda3\\envs\\fdb\\lib\\site-packages\\statsmodels\\base\\data.py:509\u001b[0m, in \u001b[0;36mPandasData._convert_endog_exog\u001b[1;34m(self, endog, exog)\u001b[0m\n\u001b[0;32m    507\u001b[0m exog \u001b[38;5;241m=\u001b[39m exog \u001b[38;5;28;01mif\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m endog\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m exog\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m--> 509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas data cast to numpy dtype of object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    510\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck input data with np.asarray(data).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(PandasData, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_convert_endog_exog(endog, exog)\n",
      "\u001b[1;31mValueError\u001b[0m: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data)."
     ]
    }
   ],
   "source": [
    "newey_west_regression(crsp_m['ret'], a, lags=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch2. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The summary statistics procedure consists of two sterps: \n",
    "# 1. For each time period $t$, certain charaacteristics of cross-sectional distribution of the given variable, $X$, are calculated.\n",
    "# 2. The time series properties of the periodic cross-sectional characteristics are calculated. \n",
    "#   The most importanr time series properties are the mean\n",
    "#   The mean means the average value of the cross-sectional characteristic over time.\n",
    "\n",
    "# Statistics for univariate distributions of the varaibles used in a study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The details of the first step are as follows. For each time period t, we calculate the cross-sectional mean, standard deviation, skewness, excess kurtosis, minimum value, median value, maximum value, and selected additional \n",
    "#percentiles of the distribution of the values of X, where each of these statistics is calculated over all available values of X in period t. \n",
    "#We let Meant be the mean, SDt denote the sample standard deviation, Ske𝑤t represent the sample skewness, Kurtt be the sample excess kurtosis, Mint be the minimum value, Mediant denote the median value, and Maxt represent the \n",
    "#maximum value of X in period t. In addition, we will record the fifth, 25th, 75th, and 95th percentiles of X in month t, which we denote P5t, P25t, P75t, and P95t, respectively. \n",
    "#Depending on the data and the objective of the study, it may be desirable to include additional percentiles of the distribution. \n",
    "#For example, if the study focuses on extreme values of X, then it may be valuable to record the first, second, third, fourth, 96th, 97th, 98th, and 99th percentiles of the distribution as well.\n",
    "#Alternatively, calculating the minimum, maximum, fifth percentile, and 95th percentile of the data may not be necessary if the data are reasonably well behaved. \n",
    "#Exactly which statistics to record and present is a decision made by the researcher, who, presumably, has a much deeper understanding of the data than could possibly be presented in a research article.\n",
    "#In addition to these statistics describing the time t cross-sectional distribution of X, we also record the number of entities for which a valid value of X is available in period t and denote this number nt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The objectives in analyzing the summary statistics are twofold. First, the summary statistics are intended to give a basic overview of the cross-sectional properties \n",
    "# of the variables that will be used in the study. This is useful for understanding the types of entities that comprise the sample. \n",
    "# Second, the summary statistics can be used to identify any potential issues that may arise when using these variables in statistical analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Time  Mean        SD     Skew  Kurt  Min  Median  Max     P5    P25  \\\n",
      "0  2024-01  13.0  2.645751 -0.59517 -1.50   10    14.0   15  10.40  12.00   \n",
      "1  2024-02  20.5  1.290994  0.00000 -1.36   19    20.5   22  19.15  19.75   \n",
      "\n",
      "     P75    P95  N  \n",
      "0  14.50  14.90  3  \n",
      "1  21.25  21.85  4  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YeonChan Kang\\AppData\\Local\\Temp\\ipykernel_72644\\734699324.py:42: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  stats_df = df.groupby(time_column).apply(calc_period_stats).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "def cal_cs_stats(df, time_column, value_column, additional_percentiles=False):\n",
    "    \"\"\"\n",
    "    Calculate cross-sectional statistics for each time period.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The data frame containing the data.\n",
    "        time_column (str): The name of the column representing time periods.\n",
    "        value_column (str): The name of the column representing the values of X.\n",
    "        additional_percentiles (list of float): Additional percentiles to calculate (optional).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A data frame containing the calculated statistics for each time period.\n",
    "    \"\"\"\n",
    "    if additional_percentiles:\n",
    "        additional_percentiles_list = [0.01, 0.02, 0.03, 0.04, 0.96, 0.97, 0.98, 0.99]\n",
    "\n",
    "    # Function to calculate the required statistics for a given period\n",
    "    def calc_period_stats(group, additional_percentiles=additional_percentiles):\n",
    "        stats = {\n",
    "            'Time': group[time_column].iloc[0],  # Time column\n",
    "            'Mean': group[value_column].mean(),\n",
    "            'SD': group[value_column].std(),\n",
    "            'Skew': skew(group[value_column], nan_policy='omit'),\n",
    "            'Kurt': kurtosis(group[value_column], nan_policy='omit'),\n",
    "            'Min': group[value_column].min(),\n",
    "            'Median': group[value_column].median(),\n",
    "            'Max': group[value_column].max(),\n",
    "            'P5': group[value_column].quantile(0.05),\n",
    "            'P25': group[value_column].quantile(0.25),\n",
    "            'P75': group[value_column].quantile(0.75),\n",
    "            'P95': group[value_column].quantile(0.95),\n",
    "            'N': group[value_column].count()\n",
    "        }\n",
    "        \n",
    "        if additional_percentiles:\n",
    "            for percentile in additional_percentiles_list:\n",
    "                stats[f'P{int(percentile*100)}'] = group[value_column].quantile(percentile)\n",
    "        \n",
    "        return pd.Series(stats)\n",
    "    \n",
    "    # Group the data by the time column and apply the function\n",
    "    stats_df = df.groupby(time_column).apply(calc_period_stats).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "# Example usage\n",
    "# Assuming `df` is your DataFrame with 'Date' as the time column and 'Value' as the value column\n",
    "df = pd.DataFrame({\n",
    "    'Date': ['2024-01', '2024-01', '2024-01', '2024-02', '2024-02', '2024-02', '2024-02'],\n",
    "    'Value': [10, 15, 14, 20, 22, 21, 19]\n",
    "})\n",
    "\n",
    "results = cal_cs_stats(df, 'Date', 'Value')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Stage\n",
    "\n",
    "caculate the time-series averages of the periodic cross-sectional values.\n",
    "    $Mean$: time-series average of the values of $Mean_t$ over all periods $t$ in the sample.\n",
    "\n",
    "Caculate the time-series means of the corss-sectional summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time-Series Averages of Cross-Sectional Statistics:\n",
      "    Mean        SD      Skew  Kurt   Min  Median   Max      P5     P25  \\\n",
      "0  16.75  1.968373 -0.297585 -1.43  14.5   17.25  18.5  14.775  15.875   \n",
      "\n",
      "      P75     P95    N  \n",
      "0  17.875  18.375  3.5  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YeonChan Kang\\AppData\\Local\\Temp\\ipykernel_72644\\734699324.py:42: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  stats_df = df.groupby(time_column).apply(calc_period_stats).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "def calculate_time_series_averages(stats_df):\n",
    "    \"\"\"\n",
    "    Calculate the time-series averages of the cross-sectional statistics.\n",
    "    \n",
    "    Args:\n",
    "        stats_df (pd.DataFrame): A data frame containing cross-sectional statistics for each time period.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: A series containing the time-series averages of the cross-sectional statistics.\n",
    "    \"\"\"\n",
    "    # Exclude the time column for averaging\n",
    "    stats_to_average = stats_df.drop(columns=['Time'])\n",
    "    \n",
    "    # Calculate the time-series averages\n",
    "    time_series_averages = stats_to_average.mean()\n",
    "    \n",
    "    df = pd.DataFrame(time_series_averages).T\n",
    "\n",
    "    return df\n",
    "\n",
    "cross_sectional_stats = cal_cs_stats(df, 'Date', 'Value')\n",
    "time_series_averages = calculate_time_series_averages(cross_sectional_stats)\n",
    "print(\"\\nTime-Series Averages of Cross-Sectional Statistics:\")\n",
    "print(time_series_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>SD</th>\n",
       "      <th>Skew</th>\n",
       "      <th>Kurt</th>\n",
       "      <th>Min</th>\n",
       "      <th>Median</th>\n",
       "      <th>Max</th>\n",
       "      <th>P5</th>\n",
       "      <th>P25</th>\n",
       "      <th>P75</th>\n",
       "      <th>P95</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.75</td>\n",
       "      <td>1.968373</td>\n",
       "      <td>-0.297585</td>\n",
       "      <td>-1.43</td>\n",
       "      <td>14.5</td>\n",
       "      <td>17.25</td>\n",
       "      <td>18.5</td>\n",
       "      <td>14.775</td>\n",
       "      <td>15.875</td>\n",
       "      <td>17.875</td>\n",
       "      <td>18.375</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Mean        SD      Skew  Kurt   Min  Median   Max      P5     P25  \\\n",
       "0  16.75  1.968373 -0.297585 -1.43  14.5   17.25  18.5  14.775  15.875   \n",
       "\n",
       "      P75     P95    N  \n",
       "0  17.875  18.375  3.5  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Mean       SD       Skew    Kurt    Min    Median    Max      P5     P25     P75     P95    N\n",
      "--  ------  -------  ---------  ------  -----  --------  -----  ------  ------  ------  ------  ---\n",
      " 0   16.75  1.96837  -0.297585   -1.43   14.5     17.25   18.5  14.775  15.875  17.875  18.375  3.5\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(time_series_averages, headers='keys', tablefmt='facy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch3. Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two correalations \n",
    "1. Pearson product moment correlation\n",
    "2. Spearmann rank correlation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson product moment correlation: most applicable when the relation between the two variabels, which we denote X and Y.\n",
    "Pearson correlation can be roughly interpreted as the signed percentage of variation in X that is related to variation in Y.\n",
    "With the sign being positive if X tends to be high when Y is high and the sign being negative whne high values of X tends to correspeond to low values of Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spearman rank correlation is most application when the relation between the variables is though to be monotonic but not neccessarily linear.\n",
    "Mesures how closely related the ordering of X is to the ordering of Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calcuate the cross-sectional correlation between the two variables in question, X and Y for each period t\n",
    "2. time-series average of the cross-sectional correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_corr(group, var1, var2, option='all'):\n",
    "    \"\"\"\n",
    "    Calculate the correlation between two variables for a given period.\n",
    "\n",
    "    Args:\n",
    "        group (pd.DataFrame): The data for the given period.\n",
    "        var1 (str): The name of the first variable.\n",
    "        var2 (str): The name of the second variable.\n",
    "        option (str): The correlation type to calculate ('pearson', 'spearman', or 'all'). Defaults to 'all'.\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: A series containing the correlation between the two variables.\n",
    "    \"\"\"\n",
    "    if option == 'pearson':\n",
    "        pearson_corr, _ = pearsonr(group[var1], group[var2])\n",
    "        return pd.Series({'Pearson': pearson_corr})\n",
    "    elif option == 'spearman':\n",
    "        spearman_corr, _ = spearmanr(group[var1], group[var2])\n",
    "        return pd.Series({'Spearman': spearman_corr})\n",
    "    elif option == 'all':\n",
    "        pearson_corr, _ = pearsonr(group[var1], group[var2])\n",
    "        spearman_corr, _ = spearmanr(group[var1], group[var2])\n",
    "        return pd.Series({'Pearson': pearson_corr, 'Spearman': spearman_corr})\n",
    "    else:\n",
    "        raise ValueError(\"Invalid option for correlation type. Choose from 'pearson', 'spearman', or 'all'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_per_corr(df, time_column, specific_date=None):\n",
    "    \"\"\"\n",
    "    Calculate Pearson and Spearman correlations for each time period for all pairs of variables.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The data frame containing the data.\n",
    "        time_column (str): The name of the column representing time periods.\n",
    "        specific_date (str or None): A specific date to filter the data. If None, calculate for all dates.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A data frame containing the Pearson and Spearman correlations for each time period.\n",
    "    \"\"\"\n",
    "    if specific_date:\n",
    "        df = df[df[time_column] == specific_date]\n",
    "    \n",
    "    variables = [col for col in df.columns if col != time_column]\n",
    "    correlations = []\n",
    "\n",
    "    for i, var1 in enumerate(variables):\n",
    "        for var2 in variables[i+1:]:\n",
    "            corr_df = df.groupby(time_column).apply(lambda group: cal_corr(group, var1, var2)).reset_index()\n",
    "            corr_df['Var1'] = var1\n",
    "            corr_df['Var2'] = var2\n",
    "            correlations.append(corr_df)\n",
    "\n",
    "    all_correlations = pd.concat(correlations, ignore_index=True)\n",
    "    return all_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_ts_avcorr(correlations_df):\n",
    "    \"\"\"\n",
    "    Calculate the time-series averages of the periodic cross-sectional correlations.\n",
    "    \n",
    "    Args:\n",
    "        correlations_df (pd.DataFrame): A data frame containing the periodic correlations.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A data frame containing the time-series average Pearson and Spearman correlations.\n",
    "    \"\"\"\n",
    "    avg_corrs = correlations_df.groupby(['Var1', 'Var2'])[['Pearson', 'Spearman']].mean().reset_index()\n",
    "    return avg_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corr_mat(avg_corrs, variables):\n",
    "    \"\"\"\n",
    "    Create a correlation matrix with Pearson correlations below the diagonal and Spearman correlations above the diagonal.\n",
    "    \n",
    "    Args:\n",
    "        avg_corrs (pd.DataFrame): A data frame containing the average correlations.\n",
    "        variables (list of str): List of column names representing the variables.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A correlation matrix.\n",
    "    \"\"\"\n",
    "    matrix = pd.DataFrame(index=variables, columns=variables)\n",
    "\n",
    "    for _, row in avg_corrs.iterrows():\n",
    "        var1, var2 = row['Var1'], row['Var2']\n",
    "        pearson, spearman = row['Pearson'], row['Spearman']\n",
    "        matrix.at[var1, var2] = f\"{spearman:.2f}\"\n",
    "        matrix.at[var2, var1] = f\"{pearson:.2f}\"\n",
    "    \n",
    "    np.fill_diagonal(matrix.values, np.nan)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Date': ['2024-01', '2024-01', '2024-01', '2024-02', '2024-02', '2024-02', '2024-02'],\n",
    "    'X': [10, 15, 14, 20, 22, 21, 19],\n",
    "    'Y': [8, 10, 12, 18, 16, 15, 14],\n",
    "    'Z': [7, 11, 13, 17, 15, 14, 13]\n",
    "})\n",
    "\n",
    "variables = ['X', 'Y', 'Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Periodic Correlations:\n",
      "      Date   Pearson  Spearman Var1 Var2\n",
      "0  2024-01  0.755929       0.5    X    Y\n",
      "1  2024-02  0.226779       0.4    X    Y\n",
      "2  2024-01  0.866025       0.5    X    Z\n",
      "3  2024-02  0.226779       0.4    X    Z\n",
      "4  2024-01  0.981981       1.0    Y    Z\n",
      "5  2024-02  1.000000       1.0    Y    Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YeonChan Kang\\AppData\\Local\\Temp\\ipykernel_72644\\3769392557.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  corr_df = df.groupby(time_column).apply(lambda group: cal_corr(group, var1, var2)).reset_index()\n",
      "C:\\Users\\YeonChan Kang\\AppData\\Local\\Temp\\ipykernel_72644\\3769392557.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  corr_df = df.groupby(time_column).apply(lambda group: cal_corr(group, var1, var2)).reset_index()\n",
      "C:\\Users\\YeonChan Kang\\AppData\\Local\\Temp\\ipykernel_72644\\3769392557.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  corr_df = df.groupby(time_column).apply(lambda group: cal_corr(group, var1, var2)).reset_index()\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Calculate periodic correlations\n",
    "periodic_correlations = cal_per_corr(df, 'Date')\n",
    "print(\"Periodic Correlations:\")\n",
    "print(periodic_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time-Series Average Correlations:\n",
      "  Var1 Var2   Pearson  Spearman\n",
      "0    X    Y  0.491354      0.45\n",
      "1    X    Z  0.546402      0.45\n",
      "2    Y    Z  0.990990      1.00\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Calculate time-series average correlations\n",
    "average_correlations = cal_ts_avcorr(periodic_correlations)\n",
    "print(\"\\nTime-Series Average Correlations:\")\n",
    "print(average_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correlation Matrix:\n",
      "      X     Y     Z\n",
      "X   NaN  0.45  0.45\n",
      "Y  0.49   NaN  1.00\n",
      "Z  0.55  0.99   NaN\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create the correlation matrix\n",
    "correlation_matrix = create_corr_mat(average_correlations, variables)\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the Spearman rank correlation is substantially larger in magnitude than the Pearson product–moment correlation, this likely indicates that there is a monotonic, but not linear, relation between the variables. \n",
    "This type of relation signals that linear regression analysis is a potentially problematic statistical technique to apply to the given variables if one of the variables is used as the dependent variable. \n",
    "\n",
    "If the Pearson product–moment correlation is substantially larger in magnitude than the Spearman rank correlation, this may indicate that there are a few extreme data points in one of the variables that are exerting a strong influence on the calculation of the Pearson product–moment correlation.\n",
    "In this case, it is possible that winsorizing one or both of the variables at a higher level will alleviate this issue. \n",
    "\n",
    "Finally, it is worth noting here that, because of the assumption of linearity in the calculation of the Pearson product–moment correlation, this measure is usually more indicative of results that will be realized using regression techniques such as Fama and MacBeth (1973) regression analysis (presented in Chapter 6). Because the Spearman rank correlation is based on the ordering of the variables, Spearman rank correlations are more likely indicative of the results of analyses that rely on the ranking, or ordering, of the variables, such as portfolio analysis (presented in Chapter 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch4. Persistence Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the variables in empirical asset pricing research are intended to capture persistent characteristics of the entities in the sample.\n",
    "This means that the characteristic of the entity that is captured by the given variable is assumed to remain reasonably stable over time.\n",
    "\n",
    "In this chapter, we discuss a technique that we call persistence analysis. We use persistence analysis to examine whether a given characteristic of the entities in our sample is in fact persistent. Persistence analysis can also be used to examine the ability of the variable in question to capture the desired characteristic of the entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Step: calculating cross-sectional correlations between the given variable X measured a certain number of periods apart.\n",
    "\n",
    "Second Step: calculating the time-series average of each of these cross-sectional correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Persistence with Entity Column:\n",
      "               t+1       t+2       t+3       t+4       t+5\n",
      "Variable                                                  \n",
      "beta     -0.012372  0.095060  0.222180  0.313634 -0.391324\n",
      "Size     -0.077550 -0.379835  0.267808  0.534534 -0.420979\n",
      "BM       -0.120001 -0.198523  0.396845 -0.014092 -0.130984\n",
      "\n",
      "Average Persistence without Entity Column:\n",
      "               t+1       t+2       t+3       t+4       t+5\n",
      "Variable                                                  \n",
      "beta     -0.084641 -0.096246  0.034608  0.035583 -0.006334\n",
      "Size     -0.124825 -0.473607  0.182061  0.390492 -0.433567\n",
      "BM       -0.118471 -0.340782  0.142714  0.067371  0.007421\n"
     ]
    }
   ],
   "source": [
    "def calculate_cross_sectional_persistence(df, time_column, value_columns, entity_column=None, max_tau=5):\n",
    "    \"\"\"\n",
    "    Calculate the cross-sectional Pearson correlations for multiple variables measured tau periods apart for multiple tau values.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The data frame containing the data.\n",
    "        time_column (str): The name of the column representing time periods.\n",
    "        value_columns (list of str): The names of the columns representing the values of the variables.\n",
    "        entity_column (str or None): The name of the column representing the entities. If None, calculate persistence without entity grouping.\n",
    "        max_tau (int): The maximum number of periods apart to measure persistence.\n",
    "    \n",
    "    Returns:\n",
    "        dict of pd.DataFrame: A dictionary containing the persistence correlations for each variable.\n",
    "    \"\"\"\n",
    "    persistence_results = {}\n",
    "\n",
    "    for value_column in value_columns:\n",
    "        # Ensure the dataframe is sorted by the time column (and entity column if provided)\n",
    "        if entity_column:\n",
    "            df = df.sort_values(by=[time_column, entity_column]).reset_index(drop=True)\n",
    "        else:\n",
    "            df = df.sort_values(by=[time_column]).reset_index(drop=True)\n",
    "        \n",
    "        # Create a dictionary to store the results\n",
    "        results = {f't+{tau}': [] for tau in range(1, max_tau + 1)}\n",
    "        results['Year'] = []\n",
    "\n",
    "        # Get unique time periods\n",
    "        unique_times = df[time_column].unique()\n",
    "\n",
    "        # Loop over each time period\n",
    "        for t in unique_times:\n",
    "            period_df = df[df[time_column] == t]\n",
    "            \n",
    "            if len(period_df) == 0:\n",
    "                continue\n",
    "\n",
    "            correlations = []\n",
    "            \n",
    "            for tau in range(1, max_tau + 1):\n",
    "                future_time = t + tau\n",
    "                \n",
    "                if future_time not in unique_times:\n",
    "                    correlations.append(np.nan)\n",
    "                    continue\n",
    "                \n",
    "                shifted_df = df[df[time_column] == future_time]\n",
    "                \n",
    "                if entity_column:\n",
    "                    # Merge on entities to ensure we only consider those with valid values for both t and t+tau\n",
    "                    merged_df = pd.merge(period_df, shifted_df, on=entity_column, suffixes=('', f'_shifted_{tau}'))\n",
    "                else:\n",
    "                    # If no entity column, just ensure both periods have data\n",
    "                    merged_df = pd.concat([period_df.reset_index(), shifted_df.reset_index()], axis=1, keys=['t', 't+tau'])\n",
    "\n",
    "                if len(merged_df) == 0:\n",
    "                    correlations.append(np.nan)\n",
    "                    continue\n",
    "                \n",
    "                # Calculate means\n",
    "                X_t = merged_df[value_column] if entity_column else merged_df[('t', value_column)]\n",
    "                X_t_tau = merged_df[f'{value_column}_shifted_{tau}'] if entity_column else merged_df[('t+tau', value_column)]\n",
    "                mean_X_t = X_t.mean()\n",
    "                mean_X_t_tau = X_t_tau.mean()\n",
    "\n",
    "                # Calculate numerator and denominator separately\n",
    "                numerator = ((X_t - mean_X_t) * (X_t_tau - mean_X_t_tau)).sum()\n",
    "                denominator = np.sqrt(((X_t - mean_X_t) ** 2).sum() * ((X_t_tau - mean_X_t_tau) ** 2).sum())\n",
    "                \n",
    "                if denominator == 0:\n",
    "                    correlations.append(np.nan)\n",
    "                else:\n",
    "                    pearson_corr = numerator / denominator\n",
    "                    correlations.append(pearson_corr)\n",
    "            \n",
    "            results['Year'].append(t)\n",
    "            for tau, corr in zip(range(1, max_tau + 1), correlations):\n",
    "                results[f't+{tau}'].append(corr)\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.set_index('Year', inplace=True)\n",
    "        persistence_results[value_column] = results_df\n",
    "\n",
    "    return persistence_results\n",
    "\n",
    "def calculate_average_persistence(persistence_results, max_tau):\n",
    "    \"\"\"\n",
    "    Calculate the time-series average of the periodic cross-sectional correlations for multiple variables.\n",
    "    \n",
    "    Args:\n",
    "        persistence_results (dict of pd.DataFrame): A dictionary containing the periodic cross-sectional correlations for each variable.\n",
    "        max_tau (int): The maximum number of periods apart to measure persistence.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A data frame containing the average persistence values for each variable and each lag.\n",
    "    \"\"\"\n",
    "    avg_persistence = {f't+{tau}': [] for tau in range(1, max_tau + 1)}\n",
    "    avg_persistence['Variable'] = []\n",
    "\n",
    "    for variable, persistence_df in persistence_results.items():\n",
    "        avg_persistence['Variable'].append(variable)\n",
    "        for tau in range(1, max_tau + 1):\n",
    "            avg_persistence[f't+{tau}'].append(persistence_df[f't+{tau}'].mean())\n",
    "\n",
    "    avg_persistence_df = pd.DataFrame(avg_persistence)\n",
    "    avg_persistence_df.set_index('Variable', inplace=True)\n",
    "    return avg_persistence_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Generate example data\n",
    "years = list(range(1994, 2011))\n",
    "entities = ['A', 'B', 'C']\n",
    "data = []\n",
    "\n",
    "for year in years:\n",
    "    for entity in entities:\n",
    "        data.append({'Year': year, 'entity': entity, 'beta': np.random.randint(10, 100),\n",
    "                     'Size': np.random.randint(50, 150), 'BM': np.random.randint(20, 120)})\n",
    "\n",
    "# Make data sparse by removing some entries for the last few years\n",
    "df = pd.DataFrame(data)\n",
    "df = df[~((df['Year'] >= 2008) & (df['entity'] == 'C'))]\n",
    "\n",
    "variables = ['beta', 'Size', 'BM']\n",
    "max_tau = 5\n",
    "\n",
    "# Calculate persistence correlations with entity column\n",
    "persistence_results_with_entity = calculate_cross_sectional_persistence(df, 'Year', variables, 'entity', max_tau)\n",
    "\n",
    "# Calculate average persistence with entity column\n",
    "average_persistence_with_entity = calculate_average_persistence(persistence_results_with_entity, max_tau)\n",
    "print(\"Average Persistence with Entity Column:\")\n",
    "print(average_persistence_with_entity)\n",
    "\n",
    "# Calculate persistence correlations without entity column\n",
    "persistence_results_without_entity = calculate_cross_sectional_persistence(df.drop(['entity'], axis=1), 'Year', variables, entity_column=None, max_tau=max_tau)\n",
    "\n",
    "# Calculate average persistence without entity column\n",
    "average_persistence_without_entity = calculate_average_persistence(persistence_results_without_entity, max_tau)\n",
    "print(\"\\nAverage Persistence without Entity Column:\")\n",
    "print(average_persistence_without_entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As might be expected, the correlations between characteristics measured at longer lags 𝜏 tend to be lower than the correlations measured at shorter lags, although this is not always the case.\n",
    "However, it is worth noting that for years $t$ toward the end of the sample, in some cases the persistence values are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although periodic cross-sectional persistence values are quite informative, they are quite difficult to read and draw conclusions from. We therefore want to summarize these periodic values more succinctly\n",
    "As with the other analyses we discuss, the main objective is to understand the persistence of the variable X in the average cross section. We therefore summarize the results by simply taking the time-series average of the periodic cross-sectional correlations. We denote these average persistence values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a higher degree of time-lagged cross-sectional correlation in the given variable is indicative of higher persistence, although there are several caveats to this that must be understood to properly make use of this technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly what qualifies as low persistence is not perfectly well defined and depends on how long the lag is between the times of measurement (𝜏), how persistent the actual \n",
    "There are generally two reasons that a variable may exhibit low or zero persistence. \n",
    "The first is that the characteristic being measured is in fact not persistent.\n",
    "The second is that the variable used to proxy for the given characteristic does a poor job at measuring the characteristic under examination.\n",
    "In this case, even if the given characteristic of the entities in the sample is highly cross-sectionally persistent, the failure of the variable X to capture cross-sectional variation in this characteristic will cause the persistence analysis to generate a low value of 𝜌𝜏 (X).\n",
    "\n",
    "In the end, however, regardless of the reason for the lack of persistence in X, if X is intended to capture a persistent characteristic of a firm, but X does not exhibit persistence, then X is not a good measure of the characteristic of interest.\n",
    "\n",
    "When the persistence analysis produces high values of 𝜌𝜏 (X), this very likely indicates both that the characteristic in question is in fact persistent and that the variable X does a good job at measuring the characteristic.\n",
    "\n",
    "There are two caveats with this statement that must be addressed. \n",
    "\n",
    "The first is that it is possible that the variable X is unintentionally capturing some persistent characteristic of the entities in the sample that is different from the characteristic that X is designed to capture. Thus, perhaps a more correct statement is that high values of 𝜌𝜏 (X) indicate that whatever characteristic is being captured by X is in fact persistent.\n",
    "If X does in fact capture the intended characteristic, then we can conclude that the characteristic is in fact persistent. \n",
    "Therefore, assuming sufficient effort has been devoted to designing the calculation of X such that it can reasonably be expected to capture the intended characteristic, high values of 𝜌𝜏 (X) are interpreted as indicating that the given characteristic is in fact persistent.\n",
    "\n",
    "The second, and much more important, caveat associated with concluding that a characteristic is persistent is that in many cases there is a mechanical reason related to the calculation of X that would result in strong cross-sectional correlation between Xt and Xt+𝜏 even if the characteristic in question is not persistent. In most cases, the reason for such a mechanical effect is that some subset of the data used to calculate X at times t and t + 𝜏 are the same. \n",
    "\n",
    "While certainly none of the variables studied in asset pricing research are perfectly persistent, different variables exhibit different degrees of persistence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the persistence of X2 at lag 𝜏 = 𝜏2 is less than that of X1 at lag 𝜏 = 𝜏1, the results are a bit more challenging to interpret, but it can generally be taken to mean that the decay in the persistence over a period of 𝜏2 − 𝜏1 periods is substantial enough to overcome any additional benefit of using 𝜏2 periods of data, compared to 𝜏1, to calculate X. If this is the case, it may also be an indication that using a full 𝜏2 periods of data is too long a measurement period because the given characteristic of the firm does in fact change substantially over periods of length 𝜏2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch5. Portfolio Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of portfolio analysis is to detect the variables for predicting cross-sectional variation of stock returns\n",
    "\n",
    "Understanding variation in characteristics of the stock across the different portfolio.\n",
    "\n",
    "Benefit of the portfolio analysis: nonparametric technique\n",
    "Drawback of the portfolio analysis: difficult to control for a large number of variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Portfolio Analysis\n",
    "\n",
    "only one sort variable X.\n",
    "The objective of the analysis is to assess the cross-sectional relation between X and outcome variable Y.\n",
    "The univariate portfolio analysis has four steps:\n",
    "1. Caculate the breakpoints that will be used to divide the sample into portfolio\n",
    "2. Using these breakpoints to form the portfolio\n",
    "3. Caculate the average value of the outcome variable Y\n",
    "4. Examine variation in these average vlaues of Y across the different portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_bp(df, time_column, value_column, num_portfolios, custom_percentiles=None):\n",
    "    \"\"\"\n",
    "    Calculate breakpoints for a given variable based on specified quantiles for univariate portfolio analysis.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The data frame containing the data.\n",
    "        time_column (str): The name of the column representing time periods.\n",
    "        value_column (str): The name of the column representing the values to calculate breakpoints for.\n",
    "        num_portfolios (int): The number of portfolios to be formed each time period.\n",
    "        custom_percentiles (list of float, optional): Custom percentiles to calculate breakpoints. If None, evenly spaced percentiles are used.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A data frame containing the breakpoints for each time period.\n",
    "    \"\"\"\n",
    "    if custom_percentiles:\n",
    "        # Ensure custom_percentiles are in the range (0, 100)\n",
    "        assert all(0 < p < 100 for p in custom_percentiles), \"Percentiles must be between 0 and 100\"\n",
    "        # Convert custom_percentiles to a fraction of 1\n",
    "        quantiles = [p / 100 for p in custom_percentiles]\n",
    "    else:\n",
    "        # Calculate evenly spaced quantiles\n",
    "        quantiles = [k / num_portfolios for k in range(1, num_portfolios)]\n",
    "    \n",
    "    breakpoints = df.groupby(time_column)[value_column].quantile(quantiles).unstack()\n",
    "    breakpoints.columns = [f'B{k+1}' for k in range(len(quantiles))]\n",
    "    return breakpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Portfolio Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_portfolios(df, time_column, value_column, breakpoints):\n",
    "    \"\"\"\n",
    "    Assign each data point to a portfolio based on the calculated breakpoints.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The data frame containing the data.\n",
    "        time_column (str): The name of the column representing time periods.\n",
    "        value_column (str): The name of the column representing the values to assign to portfolios.\n",
    "        breakpoints (pd.DataFrame): The breakpoints for each time period.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The original data frame with an additional column for the assigned portfolios.\n",
    "    \"\"\"\n",
    "    def get_portfolio(row, breakpoints):\n",
    "        period_breakpoints = breakpoints.loc[row[time_column]]\n",
    "        for i in range(len(period_breakpoints)):\n",
    "            if row[value_column] <= period_breakpoints.iloc[i]:\n",
    "                return i + 1\n",
    "        return len(period_breakpoints) + 1\n",
    "\n",
    "    df['Portfolio'] = df.apply(lambda row: get_portfolio(row, breakpoints), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Portfolio Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value-weighting is most appropriate when the entities in the analysis are stocks. In such cases, the results of equal-weighted analyses are indicative of phenomena for the average stock. The results of value-weighted analyses account for the importance, from the point of view of the stock market as a whole, of each individual stock relative to the other stocks in the given portfolio. When the outcome variables (Y) is the future stock return, the results of value-weighted analyses are generally considered to be more indica\u0002tive of return that an investor would have realized by implementing the portfolio in question. The reason for this is that value-weighted portfolios have large weights on stocks with large market capitalizations, which tend to be highly liquid. The returns of equal-weighted portfolios are potentially driven by the low-market capitalization stocks in the portfolio, which are more expensive to trade. The result is often that the average return indicated by the portfolio analysis cannot be realized by an actual investor because of transaction costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_portfolio(df, time_column, portfolio_column, outcome_column, weight_column=None):\n",
    "    \"\"\"\n",
    "    Calculate the average value of the outcome variable for each portfolio.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The data frame containing the data.\n",
    "        time_column (str): The name of the column representing time periods.\n",
    "        portfolio_column (str): The name of the column representing the assigned portfolios.\n",
    "        outcome_column (str): The name of the column representing the outcome variable.\n",
    "        weight_column (str, optional): The name of the column representing the weights. If None, equal-weighted averages are calculated.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A data frame containing the average values of the outcome variable for each portfolio.\n",
    "    \"\"\"\n",
    "    if weight_column:\n",
    "        # Calculate weighted sum and sum of weights for each portfolio\n",
    "        weighted_sum = df.groupby([time_column, portfolio_column]).apply(\n",
    "            lambda x: (x[outcome_column] * x[weight_column]).sum()\n",
    "        )\n",
    "        sum_weights = df.groupby([time_column, portfolio_column])[weight_column].sum()\n",
    "        portfolio_averages = (weighted_sum / sum_weights).unstack()\n",
    "    else:\n",
    "        portfolio_averages = df.groupby([time_column, portfolio_column])[outcome_column].mean().unstack()\n",
    "    \n",
    "    # Calculate the difference between the highest and lowest portfolios\n",
    "    portfolio_averages['Diff'] = portfolio_averages.iloc[:, -1] - portfolio_averages.iloc[:, 0]\n",
    "    \n",
    "    return portfolio_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_portfolio(df, time_column, portfolio_column, outcome_column, weight_column=None):\n",
    "    \"\"\"\n",
    "    Calculate the average value of the outcome variable for each portfolio.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The data frame containing the data.\n",
    "        time_column (str): The name of the column representing time periods.\n",
    "        portfolio_column (str): The name of the column representing the assigned portfolios.\n",
    "        outcome_column (str): The name of the column representing the outcome variable.\n",
    "        weight_column (str, optional): The name of the column representing the weights. If None, equal-weighted averages are calculated.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A data frame containing the average values of the outcome variable for each portfolio.\n",
    "    \"\"\"\n",
    "    \n",
    "    if weight_column:\n",
    "        df['weighted_outcome'] = df[outcome_column] * (df[weight_column] / df.groupby([time_column, portfolio_column])[weight_column].transform('sum'))\n",
    "        portfolio_averages = df.groupby([time_column, portfolio_column])['weighted_outcome'].sum().unstack()\n",
    "    else:\n",
    "        portfolio_averages = df.groupby([time_column, portfolio_column])[outcome_column].mean().unstack()\n",
    "    \n",
    "    # Calculate the difference between the highest and lowest portfolios\n",
    "    portfolio_averages['Diff'] = portfolio_averages.max(axis=1) - portfolio_averages.min(axis=1)\n",
    "    \n",
    "    return portfolio_averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_time_series_means(portfolio_averages):\n",
    "    \"\"\"\n",
    "    Calculate the time-series means of the period average values of the outcome variable for each portfolio and the difference portfolio.\n",
    "    \n",
    "    Args:\n",
    "        portfolio_averages (pd.DataFrame): A data frame containing the average values of the outcome variable for each portfolio.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: A series containing the time-series mean values for each portfolio and the difference portfolio.\n",
    "    \"\"\"\n",
    "    # Calculate the time-series mean for each portfolio\n",
    "    time_series_means = portfolio_averages.mean()\n",
    "    \n",
    "    # Separate the difference portfolio mean\n",
    "    diff_mean = time_series_means.pop('Diff')\n",
    "    \n",
    "    return time_series_means, diff_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_portfolio_averages(df, time_column, portfolio_column, outcome_column, weight_column=None):\n",
    "    \"\"\"\n",
    "    Calculate the average value of the outcome variable for each portfolio.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The data frame containing the data.\n",
    "        time_column (str): The name of the column representing time periods.\n",
    "        portfolio_column (str): The name of the column representing the assigned portfolios.\n",
    "        outcome_column (str): The name of the column representing the outcome variable.\n",
    "        weight_column (str, optional): The name of the column representing the weights. If None, equal-weighted averages are calculated.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A data frame containing the average values of the outcome variable for each portfolio.\n",
    "    \"\"\"\n",
    "    def portfolio_return(group, outcome_column, weight_column=None):\n",
    "        \"\"\"\n",
    "        Calculate portfolio return for a group of data.\n",
    "        \n",
    "        Args:\n",
    "            group (pd.DataFrame): The data frame containing the data for a group.\n",
    "            outcome_column (str): The name of the column representing the outcome variable.\n",
    "            weight_column (str, optional): The name of the column representing the weights. If None, equal-weighted returns are calculated.\n",
    "        \n",
    "        Returns:\n",
    "            float: The portfolio return.\n",
    "        \"\"\"\n",
    "        ret = group[outcome_column]\n",
    "        if weight_column:\n",
    "            wgt = group[weight_column]\n",
    "            wgt = wgt / wgt.sum()  # normalize weights to sum to 1\n",
    "            r_p = np.dot(ret, wgt)\n",
    "        else:\n",
    "            r_p = ret.mean()\n",
    "        return r_p\n",
    "    \n",
    "    if weight_column:\n",
    "        portfolio_averages = df.groupby([time_column, portfolio_column]).apply(\n",
    "            portfolio_return, outcome_column=outcome_column, weight_column=weight_column\n",
    "        ).unstack()\n",
    "    else:\n",
    "        portfolio_averages = df.groupby([time_column, portfolio_column])[outcome_column].mean().unstack()\n",
    "    \n",
    "    # Calculate the difference between the highest and lowest portfolios\n",
    "    portfolio_averages['Diff'] = portfolio_averages.iloc[:, -1] - portfolio_averages.iloc[:, 0]\n",
    "    \n",
    "    return portfolio_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with detailed example data\n",
    "np.random.seed(0)\n",
    "years = list(range(1994, 2011))\n",
    "entities = ['A', 'B', 'C', 'D']\n",
    "data = []\n",
    "\n",
    "# Creating example data with different market capitalizations and returns\n",
    "for year in years:\n",
    "    for entity in entities:\n",
    "        data.append({\n",
    "            'Year': year,\n",
    "            'entity': entity,\n",
    "            'X': np.random.rand(),  # Sort variable (e.g., beta)\n",
    "            'Y': np.random.rand(),  # Outcome variable (e.g., return)\n",
    "            'MktCap': np.random.rand() * 1000  # Market capitalization\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Specify the number of portfolios\n",
    "num_portfolios = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breakpoints:\n",
      "            B1        B2        B3        B4\n",
      "Year                                        \n",
      "1994  0.415929  0.459046  0.523424  0.546455\n",
      "1995  0.375678  0.610067  0.736134  0.786557\n",
      "1996  0.206043  0.325331  0.507658  0.718928\n",
      "1997  0.647221  0.672874  0.679584  0.688145\n",
      "1998  0.235526  0.265719  0.303001  0.364698\n",
      "1999  0.121349  0.142340  0.154812  0.423779\n",
      "2000  0.193359  0.319765  0.390638  0.639262\n",
      "2001  0.228718  0.368175  0.516995  0.626492\n",
      "2002  0.347573  0.578228  0.584442  0.645985\n",
      "2003  0.437996  0.627778  0.767293  0.826842\n",
      "2004  0.181724  0.360860  0.573208  0.676496\n",
      "2005  0.274727  0.412224  0.546211  0.615365\n",
      "2006  0.429115  0.653686  0.768067  0.851509\n",
      "2007  0.460900  0.739108  0.790266  0.826713\n",
      "2008  0.459730  0.742732  0.780956  0.847451\n",
      "2009  0.208613  0.252703  0.297135  0.401420\n",
      "2010  0.134927  0.247046  0.429606  0.572813\n"
     ]
    }
   ],
   "source": [
    "# Calculate breakpoints\n",
    "breakpoints = cal_bp(df, 'Year', 'X', num_portfolios)\n",
    "print(\"Breakpoints:\")\n",
    "print(breakpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data with Portfolios:\n",
      "   Year entity         X         Y      MktCap  Portfolio\n",
      "0  1994      A  0.548814  0.715189  602.763376          5\n",
      "1  1994      B  0.544883  0.423655  645.894113          4\n",
      "2  1994      C  0.437587  0.891773  963.662761          2\n",
      "3  1994      D  0.383442  0.791725  528.894920          1\n",
      "4  1995      A  0.568045  0.925597   71.036058          2\n"
     ]
    }
   ],
   "source": [
    "# Assign portfolios\n",
    "df_with_portfolios = assign_portfolios(df, 'Year', 'X', breakpoints)\n",
    "print(\"\\nData with Portfolios:\")\n",
    "print(df_with_portfolios.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Equal-Weighted Portfolio Averages:\n",
      "Portfolio         1         2         4         5      Diff\n",
      "Year                                                       \n",
      "1994       0.791725  0.891773  0.423655  0.715189 -0.076536\n",
      "1995       0.020218  0.925597  0.870012  0.461479  0.441261\n",
      "1996       0.639921  0.774234  0.018790  0.521848 -0.118073\n",
      "1997       0.616934  0.210383  0.359508  0.060225 -0.556709\n",
      "1998       0.161310  0.466311  0.363711  0.988374  0.827064\n",
      "1999       0.976459  0.196582  0.110375  0.097101 -0.879358\n",
      "2000       0.282807  0.118728  0.064147  0.604846  0.322039\n",
      "2001       0.575946  0.667410  0.265389  0.289406 -0.286540\n",
      "2002       0.677817  0.592042  0.020108  0.962189  0.284372\n",
      "2003       0.952749  0.881735  0.396506  0.699479 -0.253270\n",
      "2004       0.301575  0.618015  0.423855  0.501324  0.199750\n",
      "2005       0.298282  0.435865  0.574325  0.431418  0.133136\n",
      "2006       0.868126  0.123820  0.703889  0.714241 -0.153885\n",
      "2007       0.697429  0.866382  0.569101  0.011714 -0.685715\n",
      "2008       0.199997  0.171630  0.223925  0.704414  0.504418\n",
      "2009       0.621478  0.934214  0.398221  0.589910 -0.031568\n",
      "2010       0.434417  0.944372  0.227415  0.377752 -0.056665\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate equal-weighted portfolio averages\n",
    "equal_weighted_averages = calculate_portfolio_averages(df_with_portfolios, 'Year', 'Portfolio', 'Y')\n",
    "print(\"\\nEqual-Weighted Portfolio Averages:\")\n",
    "print(equal_weighted_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value-Weighted Portfolio Averages:\n",
      "Portfolio         1         2         4         5      Diff\n",
      "Year                                                       \n",
      "1994       0.791725  0.891773  0.423655  0.715189 -0.076536\n",
      "1995       0.020218  0.925597  0.870012  0.461479  0.441261\n",
      "1996       0.639921  0.774234  0.018790  0.521848 -0.118073\n",
      "1997       0.616934  0.210383  0.359508  0.060225 -0.556709\n",
      "1998       0.161310  0.466311  0.363711  0.988374  0.827064\n",
      "1999       0.976459  0.196582  0.110375  0.097101 -0.879358\n",
      "2000       0.282807  0.118728  0.064147  0.604846  0.322039\n",
      "2001       0.575946  0.667410  0.265389  0.289406 -0.286540\n",
      "2002       0.677817  0.592042  0.020108  0.962189  0.284372\n",
      "2003       0.952749  0.881735  0.396506  0.699479 -0.253270\n",
      "2004       0.301575  0.618015  0.423855  0.501324  0.199750\n",
      "2005       0.298282  0.435865  0.574325  0.431418  0.133136\n",
      "2006       0.868126  0.123820  0.703889  0.714241 -0.153885\n",
      "2007       0.697429  0.866382  0.569101  0.011714 -0.685715\n",
      "2008       0.199997  0.171630  0.223925  0.704414  0.504418\n",
      "2009       0.621478  0.934214  0.398221  0.589910 -0.031568\n",
      "2010       0.434417  0.944372  0.227415  0.377752 -0.056665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YeonChan Kang\\AppData\\Local\\Temp\\ipykernel_72644\\3222318002.py:37: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  portfolio_averages = df.groupby([time_column, portfolio_column]).apply(\n"
     ]
    }
   ],
   "source": [
    "# Calculate value-weighted portfolio averages\n",
    "value_weighted_averages = calculate_portfolio_averages(df_with_portfolios, 'Year', 'Portfolio', 'Y', 'MktCap')\n",
    "print(\"\\nValue-Weighted Portfolio Averages:\")\n",
    "print(value_weighted_averages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time-Series Means (Equal-Weighted):\n",
      "Portfolio\n",
      "1    0.536305\n",
      "2    0.577594\n",
      "4    0.353702\n",
      "5    0.513583\n",
      "dtype: float64\n",
      "\n",
      "Difference Mean (Equal-Weighted):\n",
      "-0.022722263559856775\n"
     ]
    }
   ],
   "source": [
    "# Calculate time-series means for equal-weighted averages\n",
    "time_series_means_eq, diff_mean_eq = calculate_time_series_means(equal_weighted_averages)\n",
    "print(\"\\nTime-Series Means (Equal-Weighted):\")\n",
    "print(time_series_means_eq)\n",
    "print(\"\\nDifference Mean (Equal-Weighted):\")\n",
    "print(diff_mean_eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time-Series Means (Value-Weighted):\n",
      "Portfolio\n",
      "1    0.536305\n",
      "2    0.577594\n",
      "4    0.353702\n",
      "5    0.513583\n",
      "dtype: float64\n",
      "\n",
      "Difference Mean (Value-Weighted):\n",
      "-0.022722263559856775\n"
     ]
    }
   ],
   "source": [
    "# Calculate time-series means for value-weighted averages\n",
    "time_series_means_val, diff_mean_val = calculate_time_series_means(value_weighted_averages)\n",
    "print(\"\\nTime-Series Means (Value-Weighted):\")\n",
    "print(time_series_means_val)\n",
    "print(\"\\nDifference Mean (Value-Weighted):\")\n",
    "print(diff_mean_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'equal_weighted_averages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     diff_mean \u001b[38;5;241m=\u001b[39m time_series_means\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m time_series_means, diff_mean\n\u001b[1;32m---> 16\u001b[0m time_series_means_eq, diff_mean_eq \u001b[38;5;241m=\u001b[39m calculate_time_series_means(\u001b[43mequal_weighted_averages\u001b[49m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTime-Series Means (Equal-Weighted):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(time_series_means_eq)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'equal_weighted_averages' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Calculate time-series means for equal-weighted averages\n",
    "def calculate_time_series_means(portfolio_averages):\n",
    "    \"\"\"\n",
    "    Calculate the time-series means of the period average values of the outcome variable for each portfolio and the difference portfolio.\n",
    "    \n",
    "    Args:\n",
    "        portfolio_averages (pd.DataFrame): A data frame containing the average values of the outcome variable for each portfolio.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: A series containing the time-series mean values for each portfolio and the difference portfolio.\n",
    "    \"\"\"\n",
    "    time_series_means = portfolio_averages.mean()\n",
    "    diff_mean = time_series_means.pop('Diff')\n",
    "    return time_series_means, diff_mean\n",
    "\n",
    "time_series_means_eq, diff_mean_eq = calculate_time_series_means(equal_weighted_averages)\n",
    "print(\"\\nTime-Series Means (Equal-Weighted):\")\n",
    "print(time_series_means_eq)\n",
    "print(\"\\nDifference Mean (Equal-Weighted):\")\n",
    "print(diff_mean_eq)\n",
    "\n",
    "time_series_means_val, diff_mean_val = calculate_time_series_means(value_weighted_averages)\n",
    "print(\"\\nTime-Series Means (Value-Weighted):\")\n",
    "print(time_series_means_val)\n",
    "print(\"\\nDifference Mean (Value-Weighted):\")\n",
    "print(diff_mean_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch6. Fama-Macbeth regression analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
